{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f976b6e-0a62-454d-8057-000f0b465630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import json\n",
    "import base64\n",
    "import pprint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9af5c6-9a1c-45ee-81a0-2ba356144bb7",
   "metadata": {},
   "source": [
    "# OpenAI API batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc2971f9-a2d0-4c67-b17f-582870b3c3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_0368ca7d9c29081d006907fa03b0b481a28d008488287a0302', created_at=1762130435.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0368ca7d9c29081d006907fa04d81081a29431d7c34fc2fd3d', summary=[], type='reasoning', status=None), ResponseOutputMessage(id='msg_0368ca7d9c29081d006907fa0809f481a2afb965aae8e19022', content=[ResponseOutputText(annotations=[], text='Under a moonlit sky, a drowsy unicorn named Luma tiptoed through a meadow of whispering stars, gathering dreams on her silver horn to share with every sleeping child before dawn.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), truncation='disabled', usage=ResponseUsage(input_tokens=17, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=174, output_tokens_details=OutputTokensDetails(reasoning_tokens=128), total_tokens=191), user=None, background=False, billing={'payer': 'openai'}, max_tool_calls=None, prompt_cache_key=None, prompt_cache_retention=None, safety_identifier=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing api\n",
    "system_instructions =  \"\"\"You are an assistant that's helping me with this research project. The structure of this project is that we have had undergraduates label traits of images manually. These traits include things like ontology, substrates, font information, text, covid-relation, confidence, and more. You are now going to be doing what they have been doing. When provided with an image, you should return a json object that describes the image according to the structure given here. The structure you should return as is provided below, along with an example. Keep in mind there can be multiple substrates (up to 4), with multiple fonts for each substrate (up to 8). Many of the images will have multiple substrates, look closely in the images for any other substrates which are clear and sharp. When annotating the text (called copy) that is in the image, ensure that you use the markdown guide that is included here. If there is a code of some type (barcode, scan code) in the image, do not include that in the json. \n",
    "    \n",
    "        This is the format you should return to me in, with comments denoting the purpose of the field for some:\n",
    "    \n",
    "        |||{return format}\n",
    "        {\n",
    "            \"substrateCount\": // the number of substrates in this image,\n",
    "            \"substrates\": [\n",
    "                {\n",
    "                    \"placement\": // refer to placement examples in screenshots of form,\n",
    "                    \"additionalNotes\": // not always necessary to include,\n",
    "                    \"thisIsntReallyASign\": // set this field to true if this doesn't really fit any of the placement categories and isn't a sign, else false,\n",
    "                    \"notASignDescription\": // use this if the previous field was true, describe the placement,\n",
    "                    \"typefaces\": [\n",
    "                        {\n",
    "                            \"typefaceStyle\": [], // can be multiple, choose from the options provided to you below,\n",
    "                            \"copy\": // make sure that the text is annotated according to the markdown guide in one of the screenshots provided to you,\n",
    "                            \"letteringOntology\": [], // refer to the OC Fonts: Codebook Descriptions & Photo Examples file for examples and descriptions of what each of these are,\n",
    "                            \"messageFunction\": // again refer to the OC Fonts: Codebook Descriptions & Photo Examples file for examples and descriptions of what each of these are,\n",
    "                            \"covidRelated\": // is this text covid related?,\n",
    "                            \"additionalNotes\": // any additional notes needed about this image\n",
    "                        }\n",
    "                    ],\n",
    "                    \"confidence\": // overall confidence in your annotation, 0 being the lowest, 5 the highest,\n",
    "                    \"confidenceReasoning\": // reasoning for confidence rating,\n",
    "                    \"additionalInfo\": // any additional info about the substrate, not always necessary to include\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        |||\n",
    "    \n",
    "        This is an example of the returned product you should give to me. This example only has one substrate but other images may have multiple.\n",
    "    \n",
    "        |||{return example}\n",
    "        {\n",
    "            \"substrateCount\": 1,\n",
    "            \"substrates\": [\n",
    "                {\n",
    "                    \"placement\": \"Window-stuck\",\n",
    "                    \"additionalNotes\": \"decal stickers on a parking meter\",\n",
    "                    \"thisIsntReallyASign\": false,\n",
    "                    \"notASignDescription\": \"\",\n",
    "                    \"typefaces\": [\n",
    "                        {\n",
    "                            \"typefaceStyle\": [\"Serif\", \"Stylized\"],\n",
    "                            \"copy\": \"Please, no food or\\\\ndrink in the store.\\\\nThank You!\",\n",
    "                            \"letteringOntology\": [\"Painted\", \"Pan-face\"],\n",
    "                            \"messageFunction\": \"Operational information\",\n",
    "                            \"covidRelated\": false,\n",
    "                            \"additionalNotes\": \"Text is center aligned, \\\\\\\"OPEN\\\\\\\" is larger than \\\\\\\"5PM DAILY\\\\\\\"\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"confidence\": 4,\n",
    "                    \"confidenceReasoning\": \"Could be painted or pan-face or both\",\n",
    "                    \"additionalInfo\": \"Image heavily cut off\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        |||\n",
    "    \n",
    "        These are the options for message function, typeface style, ontology and placements:\n",
    "\n",
    "|||    \n",
    "// --- Annotated option definitions ---\n",
    "{\n",
    "  \"typeface\": [\n",
    "    \"Serif\",        // Typefaces that have the serifs on the edges of the letters\n",
    "    \"Sans serif\",   // Typefaces that do not have the serifs on the edges of the letters\n",
    "    \"Slab serif\",   // Typefaces with thick, squared-off serifs that do not come to a point\n",
    "    \"Script\",       // Text set in a cursive or handwriting-like typeface\n",
    "    \"Stylized\",     // Fonts with a unique or decorative design that don't fit other categories\n",
    "    \"Quirky\"        // Playful or informal fonts that convey a particular feeling (e.g., Comic Sans)\n",
    "  ],\n",
    "\n",
    "  \"lettering_ontology\": [\n",
    "    \"Printed\",       // Ink letters printed onto paper, posterboard, or similar substrates\n",
    "    \"Decal\",         // A sticker/film transferred onto the surface with adhesive or heat\n",
    "    \"Painted\",       // Letters hand- or professionally-painted directly onto a surface\n",
    "    \"Pan channel\",   // Individually-cut, three-dimensional letters (mounted to a fascia)\n",
    "    \"Pan face\",      // A three-dimensional sign unit (one piece) usually mounted to a fascia\n",
    "    \"Handmade\",      // Vernacular/DIY signs made by non-professionals (may accompany other ontologies)\n",
    "    \"Embossed\",      // Text raised (3-D) above the surrounding surface\n",
    "    \"Debossed\",      // Text engraved or pressed into the surface (3-D recess)\n",
    "    \"Pen or marker\", // Text written using a pen or marker\n",
    "    \"Reader board\",  // Sign with movable letters/characters to change messages\n",
    "    \"Spray paint\",   // Letters spray-painted onto the surface (often graffiti or stenciled)\n",
    "    \"LED\",           // Signs lit by colored lights/electrical LEDs (distinct from neon)\n",
    "    \"Other electronic\", // Any electronic signage that is not LED or neon\n",
    "    \"Neon\",          // Light-up signs formed from bent, gas-filled tubes (continuous-tube look)\n",
    "    \"Tile\",          // Letters composed of or applied as tiles/mosaics\n",
    "    \"Chalk\",         // Letters written in chalk (typically on a chalkboard)\n",
    "    \"House number\",  // Address numbers made of individual shapes attached to a wall\n",
    "    \"Ghost sign\"     // Remnant/faded trace of an old sign (weathered or stained letters)\n",
    "  ],\n",
    "\n",
    "  \"placements\": [\n",
    "    \"Window-stuck\",   // Fixed to the window with adhesive (decal/sticker) and not easily moved\n",
    "    \"Window-placed\",  // Mounted on the window but removable or easily moved (not adhesive)\n",
    "    \"Awning/canopy\",  // Text on a cloth or canopy structure projecting outward over an entrance\n",
    "    \"Blade\",          // Sign that protrudes perpendicular from the wall (hangs off a mount)\n",
    "    \"Fascia\",         // Sign mounted to the building face, usually large and above eye level\n",
    "    \"Marquee\",        // Protruding, attached sign (movie-theater style) attached to a structure\n",
    "    \"Hanging\",        // Blade-style sign hanging from ceiling or roof overhang\n",
    "    \"Name-plate\",     // Panel listing multiple names/businesses arranged in a table/line\n",
    "    \"Painted wall\",   // Sign painted directly onto the wall surface (mural-like)\n",
    "    \"Freestanding\",   // Moveable signs that stand alone (A-frames, sandwich boards, posters)\n",
    "    \"Parapet\",        // Large fascia-style sign mounted on a low wall/railing at roof edge\n",
    "    \"Ground\",         // Sign affixed to or at ground/floor level\n",
    "    \"Bench\",          // Sign that is part of or attached to a bench\n",
    "    \"Flag\",           // Sign printed on flexible material (flag) secured to a pole\n",
    "    \"Pole-mounted\",   // Sign secured to a pole or post\n",
    "    \"Post and panel\", // Freestanding board/panel attached to two posts (less moveable)\n",
    "    \"Pylon\",          // Large freestanding sign > ~8 ft tall, supported by poles/structure\n",
    "    \"Banner\",         // Sign printed on flexible cloth-like material and hung up\n",
    "    \"Wall-placed\",    // Mounted on a wall but not permanently stuck (can be removed/moved)\n",
    "    \"Wall-stuck\",     // Fixed/stuck to a wall (adhesive, screwed in, or otherwise permanent)\n",
    "    \"Other-stuck\",    // Mounted to an atypical object or surface not covered above\n",
    "    \"Snipe\",          // Small sign affixed to or overlaying another sign\n",
    "    \"Graffiti\",       // Graffiti tag or stylized street art (often spray paint/paint pens)\n",
    "    \"Infrastructure\", // Municipal or utility markings (manholes, construction spraypaint, etc.)\n",
    "    \"Memorial\",       // Commemorative plaque or memorial (often on benches, plaques)\n",
    "    \"Sticker\"         // Non-sign sticker (stylized sticker placed as vandalism or sticker art)\n",
    "  ],\n",
    "\n",
    "  \"message_function\": [\n",
    "    \"Identification\",       // Identifies the name of the establishment or structure\n",
    "    \"Address\",              // Address number or the name of the address/property\n",
    "    \"Joint tenant\",         // Lists multiple businesses/people occupying the same building/area\n",
    "    \"Operational information\", // Info about business operations (hours, phone, open/closed, etc.)\n",
    "    \"Advisement/regulation\", // Regulatory or advisory signs (parking, warnings, no smoking, etc.)\n",
    "    \"Directory\",            // Lists names plus location info (where to find each listed item)\n",
    "    \"Generic information\",  // Explanatory or descriptive info (subheading, service description)\n",
    "    \"Menu of options\",      // Lists options/services/prices (menus, price boards, service lists)\n",
    "    \"Commemoration\",        // Memorial or homage (dedication plaques, historical displays)\n",
    "    \"Street name\",          // Street sign or text naming a street\n",
    "    \"Advertisement\",        // Commercial message intended to sell a product/service\n",
    "    \"Wayfinding\",           // Directional information (arrows, \"inside\", \"downstairs\", etc.)\n",
    "    \"Infrastructure\",       // Municipal/trade info for workers (IDs, construction labels, manhole tags)\n",
    "    \"Covid-related\"         // Any sign specifically related to the pandemic/COVID guidance\n",
    "  ]\n",
    "}\n",
    "// --- end annotated definitions ---\n",
    "|||\n",
    "\n",
    "This is the markdown guide for annotating the text(called copy) within images:\n",
    "|||\n",
    "~Smallcaps: {TEXT} // if some text that is part of the same typeface (thus in the same copy) is smaller \n",
    "~Italics: *text*\n",
    "~Bold: **text**\n",
    "~Bold italics: ***text***\n",
    "~Underlined: __text__\n",
    "~Illegible: [illegible]\n",
    "|||\n",
    "        \"\"\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de56d3-0d67-4aea-b370-7db4f545fdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81333bee-bea6-41d1-949f-e42d1479eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "images_path = 'images/Garden Grove'\n",
    "image_extensions = {'.JPG', '.jpg', '.jpeg'}\n",
    "output_file_name = \"batchGardenGrove\" \n",
    "batch_size = 10\n",
    "\n",
    "template = {\n",
    "    \"custom_id\": \"\",\n",
    "    \"method\": \"POST\",\n",
    "    \"url\": \"/v1/responses\",\n",
    "    \"body\": {\n",
    "        \"model\": \"gpt-5\",\n",
    "        \"instructions\": system_instructions,\n",
    "        \"input\": [\n",
    "            {\n",
    "                \"type\": \"message\",  # Changed from \"image_url\" to \"message\"\n",
    "                \"message\": {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"input_text\",\n",
    "                            \"text\": \"Use the provided system instructions to annotate this image.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",  # image_url goes INSIDE the message content\n",
    "                            \"image_url\": {\"url\": \"data:image/jpeg;base64,IMAGE_DATA_HERE\"}\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"text\": {\n",
    "            \"format\": \"text\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def encode_image(images_dir, image_name):\n",
    "    '''Encodes an image into base64 so it can be stored in a jsonl file'''\n",
    "    root_path = Path(images_dir)\n",
    "    for file in root_path.rglob(image_name):\n",
    "        with open(str(file), \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    print(f\"Skipping an image, unable to find {image_name}\")\n",
    "    return None\n",
    "\n",
    "def create_batch_object(images_path, image_extensions, output_file_name):\n",
    "    # Get all files in the directory (no subfolders)\n",
    "    image_files = [\n",
    "        f for f in os.listdir(images_path)\n",
    "        if os.path.isfile(os.path.join(images_path, f)) and \n",
    "        os.path.splitext(f)[1].lower() in image_extensions\n",
    "    ]\n",
    "    \n",
    "    output_files = []\n",
    "    print(\"Found\", len(image_files), \"images in\", images_path)\n",
    "    print(\"Processing...\")\n",
    "    \n",
    "    name_ptr = 0\n",
    "    i = 0\n",
    "    \n",
    "    while name_ptr < len(image_files):\n",
    "        with open(f\"{output_file_name}{i}.jsonl\", \"w\") as file:\n",
    "            output_files.append(f\"{output_file_name}{i}.jsonl\")\n",
    "            \n",
    "            while name_ptr < len(image_files):\n",
    "                filename = image_files[name_ptr]\n",
    "                \n",
    "                # Create a fresh copy of the template for each image\n",
    "                base64_image = encode_image(images_path, filename)\n",
    "                if not base64_image:\n",
    "                    name_ptr += 1\n",
    "                    continue\n",
    "                \n",
    "                current_template = {\n",
    "                    \"custom_id\": filename.lower(),\n",
    "                    \"method\": \"POST\",\n",
    "                    \"url\": \"/v1/responses\",\n",
    "                    \"body\": {\n",
    "                        \"model\": \"gpt-5\",\n",
    "                        \"instructions\": system_instructions,\n",
    "                        \"input\": [\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": [\n",
    "                                    {\n",
    "                                        \"type\": \"input_text\",\n",
    "                                        \"text\": \"Use the provided system instructions to annotate this image.\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"type\": \"input_image\",\n",
    "                                        \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        ],\n",
    "                        \"text\": {\"format\": {\"type\": \"text\"}}\n",
    "                    }\n",
    "                }\n",
    "                    \n",
    "                file.write(json.dumps(current_template) + '\\n')\n",
    "                name_ptr += 1\n",
    "                if name_ptr % batch_size == 0:  # 10 images per batch file\n",
    "                    break\n",
    "            \n",
    "            i += 1\n",
    "    \n",
    "    print(\"Successfully processed\", len(image_files), \"images into\", i, \"separate files\")\n",
    "    return output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738d6be-cac5-4953-9a21-f0137f63b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = create_batch_object(images_path, image_extensions, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2eaee32d-d778-4363-8141-426b5a6e5fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "def upload_single_file(filename, index, total):\n",
    "    \"\"\"Upload a single file to OpenAI\"\"\"\n",
    "    client = OpenAI(timeout=300.0, max_retries=3)\n",
    "    \n",
    "    try:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            file_obj = client.files.create(file=f, purpose=\"batch\")\n",
    "        print(f\"✓ Uploaded file {index + 1}/{total}: {filename} -> {file_obj.id}\")\n",
    "        return file_obj.id\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to upload file {index + 1}/{total}: {filename} - {e}\")\n",
    "        return None\n",
    "\n",
    "def create_single_batch(file_id, index, total):\n",
    "    \"\"\"Create a single batch job\"\"\"\n",
    "    client = OpenAI(timeout=60.0, max_retries=3)\n",
    "    \n",
    "    try:\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/responses\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"description\": \"one of many garden grove jobs\"}\n",
    "        )\n",
    "        print(f\"✓ Created batch job {index + 1}/{total}: {batch.id}\")\n",
    "        return batch\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to create batch job {index + 1}/{total}: {e}\")\n",
    "        return None\n",
    "\n",
    "def sendToOpenAI(output_files, max_workers=5):\n",
    "    \"\"\"\n",
    "    Upload files and create batch jobs using multithreading\n",
    "    \n",
    "    Args:\n",
    "        output_files: List of file paths to upload\n",
    "        max_workers: Maximum number of concurrent threads (default: 5)\n",
    "    \"\"\"\n",
    "    file_ids = []\n",
    "    \n",
    "    print(f\"Starting upload of {len(output_files)} files with {max_workers} threads...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Upload all files concurrently\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all upload tasks\n",
    "        future_to_file = {\n",
    "            executor.submit(upload_single_file, filename, i, len(output_files)): (filename, i)\n",
    "            for i, filename in enumerate(output_files)\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(future_to_file):\n",
    "            filename, index = future_to_file[future]\n",
    "            try:\n",
    "                file_id = future.result()\n",
    "                if file_id:\n",
    "                    file_ids.append((index, file_id))  # Store with index to maintain order\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Exception during upload of {filename}: {e}\")\n",
    "    \n",
    "    # Sort by original index to maintain order\n",
    "    file_ids.sort(key=lambda x: x[0])\n",
    "    file_ids = [fid for _, fid in file_ids]  # Extract just the IDs\n",
    "    \n",
    "    upload_time = time.time() - start_time\n",
    "    print(f\"\\nCompleted uploads in {upload_time:.2f} seconds\")\n",
    "    print(f\"Successfully uploaded {len(file_ids)}/{len(output_files)} files\")\n",
    "    print(f\"File IDs: {file_ids}\\n\")\n",
    "    \n",
    "    # Step 2: Create batch jobs concurrently\n",
    "    print(f\"Starting creation of {len(file_ids)} batch jobs...\")\n",
    "    batch_start = time.time()\n",
    "    \n",
    "    batches = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all batch creation tasks\n",
    "        future_to_id = {\n",
    "            executor.submit(create_single_batch, file_id, i, len(file_ids)): (file_id, i)\n",
    "            for i, file_id in enumerate(file_ids)\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(future_to_id):\n",
    "            file_id, index = future_to_id[future]\n",
    "            try:\n",
    "                batch = future.result()\n",
    "                if batch:\n",
    "                    batches.append((index, batch))  # Store with index to maintain order\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Exception during batch creation for {file_id}: {e}\")\n",
    "    \n",
    "    # Sort by original index to maintain order\n",
    "    batches.sort(key=lambda x: x[0])\n",
    "    batches = [batch for _, batch in batches]  # Extract just the batch objects\n",
    "    \n",
    "    batch_time = time.time() - batch_start\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nCompleted batch creation in {batch_time:.2f} seconds\")\n",
    "    print(f\"Successfully created {len(batches)}/{len(file_ids)} batch jobs\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    return batches\n",
    "\n",
    "\n",
    "# Alternative: Simpler version with less verbose output\n",
    "def sendToOpenAI_simple(output_files, max_workers=5):\n",
    "    \"\"\"Simplified multithreaded version with less output\"\"\"\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    \n",
    "    client = OpenAI(timeout=300.0, max_retries=3)\n",
    "    \n",
    "    def upload_file(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return client.files.create(file=f, purpose=\"batch\").id\n",
    "    \n",
    "    def create_batch(file_id):\n",
    "        return client.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/responses\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"description\": \"one of many garden grove jobs\"}\n",
    "        )\n",
    "    \n",
    "    print(f\"Uploading {len(output_files)} files...\")\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        file_ids = list(executor.map(upload_file, output_files))\n",
    "    print(f\"✓ Uploaded {len(file_ids)} files\")\n",
    "    \n",
    "    print(f\"Creating {len(file_ids)} batch jobs...\")\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        batches = list(executor.map(create_batch, file_ids))\n",
    "    print(f\"✓ Created {len(batches)} batch jobs\")\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3dc4fafd-369a-4f45-b6a1-35b65992f2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchGardenGrove0.jsonl: 34.37 MB\n",
      "batchGardenGrove1.jsonl: 36.00 MB\n",
      "batchGardenGrove2.jsonl: 35.93 MB\n",
      "batchGardenGrove3.jsonl: 33.19 MB\n",
      "batchGardenGrove4.jsonl: 33.62 MB\n",
      "batchGardenGrove5.jsonl: 34.15 MB\n",
      "batchGardenGrove6.jsonl: 38.93 MB\n",
      "batchGardenGrove7.jsonl: 31.96 MB\n",
      "batchGardenGrove8.jsonl: 41.39 MB\n",
      "batchGardenGrove9.jsonl: 40.87 MB\n",
      "batchGardenGrove10.jsonl: 34.95 MB\n",
      "batchGardenGrove11.jsonl: 39.15 MB\n",
      "batchGardenGrove12.jsonl: 31.58 MB\n",
      "batchGardenGrove13.jsonl: 31.46 MB\n",
      "batchGardenGrove14.jsonl: 35.75 MB\n",
      "batchGardenGrove15.jsonl: 26.71 MB\n"
     ]
    }
   ],
   "source": [
    "for file in output_files:\n",
    "    size_mb = os.path.getsize(file) / (1024 * 1024)\n",
    "    print(f\"{file}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f47ccc3-b2ce-435a-8480-0876149685b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload of 16 files with 5 threads...\n",
      "✓ Uploaded file 4/16: batchGardenGrove3.jsonl -> file-YcGvAWzkFe8YkN6LdBdB96\n",
      "✓ Uploaded file 3/16: batchGardenGrove2.jsonl -> file-V8GvSf7M4EwRP5EfofU7nU\n",
      "✓ Uploaded file 1/16: batchGardenGrove0.jsonl -> file-YQhZZDwJ772AAkvFT93jNg\n",
      "✓ Uploaded file 5/16: batchGardenGrove4.jsonl -> file-51FcUiu4HX6p7ScCjUXNEx\n",
      "✓ Uploaded file 2/16: batchGardenGrove1.jsonl -> file-YSgqNSuPxHMBPTuYirdf88\n",
      "✓ Uploaded file 6/16: batchGardenGrove5.jsonl -> file-XQhzY5jVSUi5CQFeSUgkYX\n",
      "✓ Uploaded file 7/16: batchGardenGrove6.jsonl -> file-FLB7PFcxxdgjGhd6q8VxJ4\n",
      "✓ Uploaded file 8/16: batchGardenGrove7.jsonl -> file-Xhvru5pVKdTMQc9yrm8p2i\n",
      "✓ Uploaded file 10/16: batchGardenGrove9.jsonl -> file-YYUKdSL8HQhZ63W7P1ckPv\n",
      "✓ Uploaded file 9/16: batchGardenGrove8.jsonl -> file-XbVAV5oEPdC7ATSokLRX7J\n",
      "✗ Failed to upload file 12/16: batchGardenGrove11.jsonl - Connection error.\n",
      "✗ Failed to upload file 13/16: batchGardenGrove12.jsonl - Connection error.\n",
      "✓ Uploaded file 14/16: batchGardenGrove13.jsonl -> file-Aw8hHvBjfSimyYWbiA4UjW\n",
      "✓ Uploaded file 16/16: batchGardenGrove15.jsonl -> file-BwxPQNhVFZsn81HvZBs4Zx\n",
      "✓ Uploaded file 11/16: batchGardenGrove10.jsonl -> file-VYKBhiTPTT2mMR9NFSvNEt\n",
      "✓ Uploaded file 15/16: batchGardenGrove14.jsonl -> file-QWzCTDfPpwUMFGFWUgzYXJ\n",
      "\n",
      "Completed uploads in 163.94 seconds\n",
      "Successfully uploaded 14/16 files\n",
      "File IDs: ['file-YQhZZDwJ772AAkvFT93jNg', 'file-YSgqNSuPxHMBPTuYirdf88', 'file-V8GvSf7M4EwRP5EfofU7nU', 'file-YcGvAWzkFe8YkN6LdBdB96', 'file-51FcUiu4HX6p7ScCjUXNEx', 'file-XQhzY5jVSUi5CQFeSUgkYX', 'file-FLB7PFcxxdgjGhd6q8VxJ4', 'file-Xhvru5pVKdTMQc9yrm8p2i', 'file-XbVAV5oEPdC7ATSokLRX7J', 'file-YYUKdSL8HQhZ63W7P1ckPv', 'file-VYKBhiTPTT2mMR9NFSvNEt', 'file-Aw8hHvBjfSimyYWbiA4UjW', 'file-QWzCTDfPpwUMFGFWUgzYXJ', 'file-BwxPQNhVFZsn81HvZBs4Zx']\n",
      "\n",
      "Starting creation of 14 batch jobs...\n",
      "✓ Created batch job 4/14: batch_6908180b87208190a60e1e00dbb1f451\n",
      "✓ Created batch job 1/14: batch_6908180b85cc8190b04e44c0cb2d0a13\n",
      "✓ Created batch job 2/14: batch_6908180b92588190a4623a0fb184e2a2\n",
      "✓ Created batch job 6/14: batch_6908180bd1108190b3842aa6e9c736c0\n",
      "✓ Created batch job 7/14: batch_6908180bda948190951ee0bb7fe3fe26\n",
      "✓ Created batch job 5/14: batch_6908180bb3748190b4316aab6ce519a5\n",
      "✓ Created batch job 8/14: batch_6908180bf4b881909e8aef42f9356545\n",
      "✓ Created batch job 3/14: batch_6908180bab1c81909a26545519f09e8a\n",
      "✓ Created batch job 12/14: batch_6908180c80888190a4d99acd825e2852\n",
      "✓ Created batch job 10/14: batch_6908180c79fc8190803618d5d1996b90\n",
      "✓ Created batch job 13/14: batch_6908180c7b208190beb14120cad43511\n",
      "✓ Created batch job 11/14: batch_6908180c8a0c81909f79e873cddd9f9c\n",
      "✓ Created batch job 14/14: batch_6908180cd6948190ad73cd2b0d036860\n",
      "✓ Created batch job 9/14: batch_6908181bf0c4819095d7f8b8733e0d9d\n",
      "\n",
      "Completed batch creation in 16.86 seconds\n",
      "Successfully created 14/14 batch jobs\n",
      "Total time: 180.80 seconds\n"
     ]
    }
   ],
   "source": [
    "batches = sendToOpenAI(output_files, max_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f3be188-00b9-424d-bb07-3c5faeb1102e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Single Image Batch Upload ===\n",
      "\n",
      "Creating test batch with: GardenGrove148.jpeg\n",
      "Created test_single_image.jsonl: 3.13 MB\n",
      "\n",
      "=== Uploading test_single_image.jsonl ===\n",
      "\n",
      "Upload attempt 1/3...\n",
      "✓ Upload successful! File ID: file-KRsdXvdaUHNW8UMHr2Us93\n",
      "\n",
      "=== Creating batch job ===\n",
      "\n",
      "✓ Batch job created: batch_690803417ed48190aed85cc145ab9391\n",
      "\n",
      "✓✓✓ SUCCESS! Batch ID: batch_690803417ed48190aed85cc145ab9391\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "images_path = 'images/Garden Grove'\n",
    "image_extensions = {'.JPG', '.jpg', '.jpeg'}\n",
    "system_instructions = \"\"\"Your system instructions here...\"\"\"\n",
    "\n",
    "def encode_image(images_dir, image_name):\n",
    "    root_path = Path(images_dir)\n",
    "    for file in root_path.rglob(image_name):\n",
    "        with open(str(file), \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    print(f\"Skipping an image, unable to find {image_name}\")\n",
    "    return None\n",
    "\n",
    "def create_single_test_batch():\n",
    "    \"\"\"Create a batch file with just ONE image for testing\"\"\"\n",
    "    image_files = [\n",
    "        f for f in os.listdir(images_path)\n",
    "        if os.path.isfile(os.path.join(images_path, f)) and \n",
    "        os.path.splitext(f)[1].lower() in image_extensions\n",
    "    ]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No images found!\")\n",
    "        return None\n",
    "    \n",
    "    # Take just the first image\n",
    "    filename = image_files[0]\n",
    "    print(f\"Creating test batch with: {filename}\")\n",
    "    \n",
    "    base64_image = encode_image(images_path, filename)\n",
    "    if not base64_image:\n",
    "        return None\n",
    "    \n",
    "    # CORRECTED FORMAT: input should be an array with role/content structure\n",
    "    template = {\n",
    "        \"custom_id\": filename.lower(),\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/responses\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-5\",\n",
    "            \"instructions\": system_instructions,\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"input_text\",\n",
    "                            \"text\": \"Use the provided system instructions to annotate this image.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"input_image\",  # Changed from \"image_url\" to \"input_image\"\n",
    "                            \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"text\": {\"format\": {\"type\": \"text\"}}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_file = \"test_single_image.jsonl\"\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(json.dumps(template) + '\\n')\n",
    "    \n",
    "    # Check file size\n",
    "    size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    print(f\"Created {output_file}: {size_mb:.2f} MB\")\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "def upload_with_retry(filename, max_attempts=3):\n",
    "    \"\"\"Upload file with exponential backoff retry\"\"\"\n",
    "    client = OpenAI(\n",
    "        timeout=300.0,  # 5 minute timeout\n",
    "        max_retries=0   # We'll handle retries manually\n",
    "    )\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            print(f\"Upload attempt {attempt + 1}/{max_attempts}...\")\n",
    "            with open(filename, \"rb\") as f:\n",
    "                file_obj = client.files.create(\n",
    "                    file=f,\n",
    "                    purpose=\"batch\"\n",
    "                )\n",
    "            print(f\"✓ Upload successful! File ID: {file_obj.id}\")\n",
    "            return file_obj.id\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Upload attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_attempts - 1:\n",
    "                wait_time = (2 ** attempt)  # Exponential backoff: 1, 2, 4 seconds\n",
    "                print(f\"Waiting {wait_time} seconds before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(\"All upload attempts failed!\")\n",
    "                raise\n",
    "    return None\n",
    "\n",
    "def create_batch_job(file_id):\n",
    "    \"\"\"Create the batch job\"\"\"\n",
    "    client = OpenAI(timeout=60.0)\n",
    "    \n",
    "    try:\n",
    "        batch = client.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/responses\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"description\": \"test batch job\"}\n",
    "        )\n",
    "        print(f\"✓ Batch job created: {batch.id}\")\n",
    "        return batch\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to create batch job: {e}\")\n",
    "        raise\n",
    "\n",
    "# Run the test\n",
    "print(\"=== Testing Single Image Batch Upload ===\\n\")\n",
    "\n",
    "# Step 1: Create test file\n",
    "test_file = create_single_test_batch()\n",
    "if not test_file:\n",
    "    print(\"Failed to create test file\")\n",
    "else:\n",
    "    print(f\"\\n=== Uploading {test_file} ===\\n\")\n",
    "    \n",
    "    # Step 2: Upload with retry\n",
    "    try:\n",
    "        file_id = upload_with_retry(test_file)\n",
    "        \n",
    "        # Step 3: Create batch job\n",
    "        if file_id:\n",
    "            print(f\"\\n=== Creating batch job ===\\n\")\n",
    "            batch = create_batch_job(file_id)\n",
    "            print(f\"\\n✓✓✓ SUCCESS! Batch ID: {batch.id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗✗✗ FAILED: {e}\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Check your OPENAI_API_KEY is set correctly\")\n",
    "        print(\"2. Try: pip install --upgrade openai certifi\")\n",
    "        print(\"3. Check your network/firewall settings\")\n",
    "        print(\"4. Try using a VPN if you're in a restricted network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f44fa1-6833-47fe-bada-a41b91959739",
   "metadata": {},
   "source": [
    "# Gemini API batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb73932-cbe3-440c-8268-e8701bc51fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# If needed:\n",
    "%pip -q install -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af412797-1ef1-42d5-89f8-295eeed1a304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fd04375-cbfd-44a0-bd2a-240140d10c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Gemini JSONL files: ['gemini_batch_0.jsonl', 'gemini_batch_1.jsonl', 'gemini_batch_2.jsonl', 'gemini_batch_3.jsonl']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m jobs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m jsonl \u001b[38;5;129;01min\u001b[39;00m gemini_jsonls:\n\u001b[0;32m---> 78\u001b[0m     uploaded \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mjsonl\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUploadFileConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjsonl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.5-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     83\u001b[0m         job \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     84\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     85\u001b[0m             src\u001b[38;5;241m=\u001b[39muploaded\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m     86\u001b[0m             config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathlib\u001b[38;5;241m.\u001b[39mPath(jsonl)\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     87\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/cs178/lib/python3.10/site-packages/google/genai/files.py:494\u001b[0m, in \u001b[0;36mFiles.upload\u001b[0;34m(self, file, config)\u001b[0m\n\u001b[1;32m    488\u001b[0m   fs_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(file)\n\u001b[1;32m    489\u001b[0m   return_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mupload_file(\n\u001b[1;32m    490\u001b[0m       fs_path, upload_url, file_obj\u001b[38;5;241m.\u001b[39msize_bytes, http_options\u001b[38;5;241m=\u001b[39mhttp_options\n\u001b[1;32m    491\u001b[0m   )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m types\u001b[38;5;241m.\u001b[39mFile\u001b[38;5;241m.\u001b[39m_from_response(\n\u001b[0;32m--> 494\u001b[0m     response\u001b[38;5;241m=\u001b[39m\u001b[43mreturn_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    495\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mconfig_model\u001b[38;5;241m.\u001b[39mmodel_dump() \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;28;01melse\u001b[39;00m {},\n\u001b[1;32m    496\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'file'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, pathlib, base64\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# === CONFIG ===\n",
    "images_path = \"images/Garden Grove\"   # <-- set your folder here\n",
    "image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.gif'}\n",
    "output_prefix = \"gemini_batch_\"       # output JSONL name prefix\n",
    "submit_to_gemini = True              # True = upload + batch submit\n",
    "# =============\n",
    "\n",
    "def find_images(folder, exts):\n",
    "    p = pathlib.Path(folder)\n",
    "    return [str(f) for f in sorted(p.iterdir()) if f.suffix.lower() in exts and f.is_file()]\n",
    "\n",
    "def base64_encode_file(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"ascii\")\n",
    "\n",
    "def mime_from_suffix(suffix):\n",
    "    suf = suffix.lower().lstrip(\".\")\n",
    "    return {\n",
    "        \"jpg\": \"image/jpeg\", \"jpeg\": \"image/jpeg\",\n",
    "        \"png\": \"image/png\", \"webp\": \"image/webp\", \"gif\": \"image/gif\"\n",
    "    }.get(suf, \"application/octet-stream\")\n",
    "\n",
    "def build_gemini_jsonl_from_folder(images_path, out_prefix, system_text):\n",
    "    images = find_images(images_path, image_extensions)\n",
    "    if not images:\n",
    "        print(\"No images found in\", images_path)\n",
    "        return []\n",
    "    jsonl_paths = []\n",
    "    N = 40  # images per JSONL file\n",
    "    for idx, img in enumerate(images):\n",
    "        out_index = idx // N\n",
    "        out_path = f\"{out_prefix}{out_index}.jsonl\"\n",
    "        if not pathlib.Path(out_path).exists():\n",
    "            open(out_path, \"w\").close()\n",
    "        mime = mime_from_suffix(pathlib.Path(img).suffix)\n",
    "        b64 = base64_encode_file(img)\n",
    "        key = pathlib.Path(img).name\n",
    "        req = {\n",
    "            \"key\": key,\n",
    "            \"request\": {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"parts\": [\n",
    "                            {\"text\": \"Use the provided system instructions to annotate this image.\"},\n",
    "                            {\"inline_data\": {\"mime_type\": mime, \"data\": b64}}\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"generation_config\": {\"temperature\": 0},\n",
    "                \"config\": {\n",
    "                    \"system_instruction\": {\"parts\": [{\"text\": system_text}]}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        with open(out_path, \"a\") as fout:\n",
    "            fout.write(json.dumps(req) + \"\\n\")\n",
    "        if out_path not in jsonl_paths:\n",
    "            jsonl_paths.append(out_path)\n",
    "    print(\"Created Gemini JSONL files:\", jsonl_paths)\n",
    "    return jsonl_paths\n",
    "\n",
    "gemini_jsonls = build_gemini_jsonl_from_folder(images_path, output_prefix, system_instructions)\n",
    "\n",
    "# Optionally upload + create batch jobs\n",
    "if submit_to_gemini and gemini_jsonls:\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"GEMINI_API_KEY/GOOGLE_API_KEY not set — skipping upload/submission.\")\n",
    "    else:\n",
    "        client = genai.Client()\n",
    "        jobs = {}\n",
    "        for jsonl in gemini_jsonls:\n",
    "            uploaded = client.files.upload(\n",
    "                file=f\"./{jsonl}\",\n",
    "                config=types.UploadFileConfig(display_name=pathlib.Path(jsonl).name, mime_type=\"jsonl\")\n",
    "            )\n",
    "            for model in (\"gemini-2.5-flash\", \"gemini-2.5-pro\"):\n",
    "                job = client.batches.create(\n",
    "                    model=model,\n",
    "                    src=uploaded.name,\n",
    "                    config={\"display_name\": f\"{pathlib.Path(jsonl).name} -> {model}\"}\n",
    "                )\n",
    "                jobs.setdefault(model, []).append(job.name)\n",
    "                print(f\"[{model}] created batch job: {job.name}\")\n",
    "\n",
    "        print(\"Submitted jobs (per model):\")\n",
    "        for m, lst in jobs.items():\n",
    "            print(m, len(lst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d611b5-2c2a-4eda-b54e-e878ad5a5ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81b02940-10ef-478f-9f8a-b7bd815c25eb",
   "metadata": {},
   "source": [
    "# Claude API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61974537-30f4-47d9-b139-a4c51d324aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 158 Claude requests from folder\n",
      "Chunked into 7 POST(s) (25 per batch).\n",
      "Wrote Claude JSONL chunk files: ['claude_batch_0.jsonl', 'claude_batch_1.jsonl', 'claude_batch_2.jsonl', 'claude_batch_3.jsonl', 'claude_batch_4.jsonl', 'claude_batch_5.jsonl', 'claude_batch_6.jsonl']\n"
     ]
    }
   ],
   "source": [
    "# === CLAUDE BATCH BUILDER (25 IMAGES PER SUBMISSION) ===\n",
    "# Requirements: anthropic package, requests, ANTHROPIC_API_KEY\n",
    "# %pip -q install anthropic requests\n",
    "\n",
    "import os, json, time, base64, requests\n",
    "from pathlib import Path\n",
    "import anthropic\n",
    "\n",
    "# === CONFIG ===\n",
    "images_path = \"images/Garden Grove\"   # <-- set this\n",
    "image_extensions = {'.jpg', '.jpeg', '.png', '.webp', '.gif'}\n",
    "output_prefix = \"claude_batch_\"       # JSONL files will be written as claude_batch_0.jsonl ...\n",
    "MAX_REQS_PER_BATCH = 25               # <-- limit by number of records\n",
    "submit_to_claude = False              # True = upload and poll results\n",
    "# =================\n",
    "\n",
    "def find_images(folder, exts):\n",
    "    p = Path(folder)\n",
    "    return [str(f) for f in sorted(p.iterdir()) if f.suffix.lower() in exts and f.is_file()]\n",
    "\n",
    "def base64_encode_file(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"ascii\")\n",
    "\n",
    "def mime_from_suffix(suffix):\n",
    "    suf = suffix.lower().lstrip(\".\")\n",
    "    return {\n",
    "        \"jpg\": \"image/jpeg\", \"jpeg\": \"image/jpeg\",\n",
    "        \"png\": \"image/png\", \"webp\": \"image/webp\", \"gif\": \"image/gif\"\n",
    "    }.get(suf, \"application/octet-stream\")\n",
    "\n",
    "def build_claude_requests_from_folder(images_path, system_text):\n",
    "    imgs = find_images(images_path, image_extensions)\n",
    "    if not imgs:\n",
    "        print(\"No images found:\", images_path)\n",
    "        return []\n",
    "    requests_list = []\n",
    "    for i, img in enumerate(imgs, start=1):\n",
    "        mime = mime_from_suffix(Path(img).suffix)\n",
    "        b64 = base64_encode_file(img)\n",
    "        custom_id = Path(img).name\n",
    "        params = {\n",
    "            \"model\": \"claude-sonnet-4-5\",\n",
    "            \"system\": system_text,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Use the provided system instructions to annotate this image.\"},\n",
    "                        {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": mime, \"data\": b64}}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": 2048\n",
    "        }\n",
    "        requests_list.append({\"custom_id\": custom_id, \"params\": params})\n",
    "    print(f\"Prepared {len(requests_list)} Claude requests from folder\")\n",
    "    return requests_list\n",
    "\n",
    "def chunk_requests(requests_list, max_items=MAX_REQS_PER_BATCH):\n",
    "    \"\"\"Chunk purely by number of items (no size consideration).\"\"\"\n",
    "    return [requests_list[i:i + max_items] for i in range(0, len(requests_list), max_items)]\n",
    "\n",
    "def write_jsonl_chunks(chunks, prefix):\n",
    "    out_files = []\n",
    "    for i, c in enumerate(chunks, start=1):\n",
    "        p = f\"{prefix}{i-1}.jsonl\"\n",
    "        with open(p, \"w\") as f:\n",
    "            for item in c:\n",
    "                f.write(json.dumps(item) + \"\\n\")\n",
    "        out_files.append(p)\n",
    "    print(\"Wrote Claude JSONL chunk files:\", out_files)\n",
    "    return out_files\n",
    "\n",
    "def submit_claude_chunks_and_wait(chunks, model_alias=\"claude-sonnet-4-5\"):\n",
    "    api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"ANTHROPIC_API_KEY not set — skipping submission.\")\n",
    "        return []\n",
    "    client = anthropic.Anthropic(api_key=api_key)\n",
    "    results = []\n",
    "    for i, chunk in enumerate(chunks, start=1):\n",
    "        batch = client.messages.batches.create(requests=chunk)\n",
    "        print(f\"Submitted chunk {i}: {batch.id}\")\n",
    "        while True:\n",
    "            b = client.messages.batches.retrieve(batch.id)\n",
    "            status = getattr(b, \"processing_status\", None)\n",
    "            print(f\"  chunk {i} status: {status}\")\n",
    "            if status == \"ended\" and getattr(b, \"results_url\", None):\n",
    "                break\n",
    "            if status in (\"failed\", \"cancelled\", \"expired\"):\n",
    "                print(\"Chunk failed/ended with status:\", status)\n",
    "                break\n",
    "            time.sleep(5)\n",
    "        results_url = getattr(b, \"results_url\", None)\n",
    "        if results_url:\n",
    "            headers = {\"x-api-key\": api_key, \"anthropic-version\": \"2023-06-01\"}\n",
    "            out_path = f\"{model_alias}_results_part{i:03d}.jsonl\"\n",
    "            with requests.get(results_url, headers=headers, stream=True) as r:\n",
    "                r.raise_for_status()\n",
    "                with open(out_path, \"wb\") as f:\n",
    "                    for chunk_bytes in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk_bytes)\n",
    "            print(\"Saved results to\", out_path)\n",
    "            results.append(out_path)\n",
    "    return results\n",
    "\n",
    "# Run\n",
    "reqs = build_claude_requests_from_folder(images_path, system_instructions)\n",
    "if reqs:\n",
    "    chunks = chunk_requests(reqs)\n",
    "    print(f\"Chunked into {len(chunks)} POST(s) ({MAX_REQS_PER_BATCH} per batch).\")\n",
    "    jsonl_files = write_jsonl_chunks(chunks, output_prefix)\n",
    "    if submit_to_claude:\n",
    "        results = submit_claude_chunks_and_wait(chunks)\n",
    "        print(\"Result files downloaded:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c110ea-8e65-4085-bf30-a76bc65bdd01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs178",
   "language": "python",
   "name": "cs178"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
